{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22315,"status":"ok","timestamp":1745389109764,"user":{"displayName":"Đức đẹp trai Anh","userId":"04958906416519110938"},"user_tz":-420},"id":"bzqLNoydfY69","outputId":"b21f8571-c79f-4baa-a397-6b4d2fda12cf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2t1I9iwtNiIT"},"outputs":[],"source":["import torch\n","import torch.optim as optim\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from PIL import Image\n","import os\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","from transformers import ViTForImageClassification, ViTConfig\n","from sklearn.metrics import classification_report, confusion_matrix\n","import seaborn as sns\n","import numpy as np\n","from collections import Counter\n","from sklearn.model_selection import train_test_split\n"]},{"cell_type":"markdown","metadata":{"id":"PhFwDcr_NiIT"},"source":["1. CHUẨN BỊ DATASET"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7jqPvNdnNiIT"},"outputs":[],"source":["class ActionDataset(Dataset):\n","    def __init__(self, root_dir, transform=None):\n","        self.root_dir = root_dir\n","        self.transform = transform\n","        # Lấy danh sách các lớp từ tên thư mục\n","        self.classes = sorted(os.listdir(root_dir))\n","        # Tạo dictionary ánh xạ tên lớp -> số index\n","        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n","        # Load tất cả đường dẫn ảnh và nhãn tương ứng\n","        self.images = self._load_images()\n","\n","    def _load_images(self):\n","        #Hàm helper để load tất cả ảnh và nhãn\n","        images = []\n","        for class_name in self.classes:\n","            class_dir = os.path.join(self.root_dir, class_name)\n","            for img_name in os.listdir(class_dir):\n","                img_path = os.path.join(class_dir, img_name)\n","                # Lưu cặp (đường dẫn ảnh, index lớp)\n","                images.append((img_path, self.class_to_idx[class_name]))\n","        return images\n","\n","    def __len__(self):\n","        #Trả về số lượng ảnh trong dataset\"\"\"\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        #Lấy ảnh và nhãn tại vị trí idx\n","        img_path, label = self.images[idx]\n","        # Mở ảnh và chuyển sang RGB (phòng trường hợp ảnh grayscale)\n","        image = Image.open(img_path).convert('RGB')\n","\n","        # Áp dụng các phép biến đổi nếu có\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, label\n"]},{"cell_type":"markdown","metadata":{"id":"Z9kBnsFtNiIU"},"source":["2. ĐỊNH NGHĨA TRANSFORM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1mf_VL5mNiIU"},"outputs":[],"source":["# Biến đổi ảnh cho tập huấn luyện (data augmentation mạnh để tăng khả năng khái quát)\n","train_transform = transforms.Compose([\n","    transforms.Resize(256),  # Resize ảnh về kích thước 256 (giữ nguyên tỷ lệ cạnh dài)\n","\n","    transforms.RandomResizedCrop(224, scale=(0.6, 1.0)),  # Cắt ngẫu nhiên vùng 224x224 từ ảnh gốc đã resize\n","    # scale=(0.6, 1.0): tỉ lệ vùng cắt so với ảnh gốc từ 60% đến 100%\n","    # -> tạo sự đa dạng về kích thước và vị trí ảnh input\n","\n","    transforms.RandomHorizontalFlip(p=0.5),  # Lật ngang ảnh ngẫu nhiên 50%\n","    # Giúp mô hình học được tính bất biến khi đối tượng bị lật\n","\n","    transforms.RandomVerticalFlip(p=0.2),  # Lật dọc ảnh với xác suất 20%\n","    # Ít phổ biến hơn nhưng vẫn có ích trong một số bài toán (ví dụ: tay, yoga...)\n","\n","    transforms.RandomApply(\n","        [transforms.ColorJitter(0.4, 0.4, 0.4, 0.2)], p=0.8\n","    ),\n","    # Áp dụng biến đổi màu sắc (ColorJitter) với xác suất 80%\n","    # Điều chỉnh độ sáng, tương phản, bão hòa và hue ngẫu nhiên\n","    # -> giúp mô hình chống lại sự thay đổi ánh sáng môi trường\n","\n","    transforms.RandomGrayscale(p=0.2),  # Chuyển ảnh thành grayscale (đen trắng) với xác suất 20%\n","    # Bắt buộc mô hình học cả đặc trưng hình dạng mà không phụ thuộc màu sắc\n","\n","    transforms.RandomRotation(15),  # Xoay ảnh ngẫu nhiên trong khoảng ±15 độ\n","    # Tạo ảnh biến thể về góc nhìn, tăng khả năng tổng quát\n","\n","    transforms.ToTensor(),  # Chuyển ảnh PIL thành tensor (C x H x W) và chuẩn hóa pixel [0, 1]\n","\n","    transforms.Normalize(\n","        mean=[0.485, 0.456, 0.406],  # Trung bình kênh RGB (ImageNet)\n","        std=[0.229, 0.224, 0.225]    # Độ lệch chuẩn kênh RGB (ImageNet)\n","    ),\n","    # Chuẩn hóa giá trị ảnh theo phân phối của ImageNet — quan trọng vì mô hình ViT pretrained trên ImageNet\n","])\n","\n","# Biến đổi ảnh cho tập validation và test (không dùng augmentation để giữ đánh giá chính xác)\n","test_transform = transforms.Compose([\n","    transforms.Resize(256),  # Resize chiều dài ngắn nhất về 256\n","    transforms.CenterCrop(224),  # Cắt vùng trung tâm kích thước 224x224\n","    # Đảm bảo input có cùng kích thước với ảnh khi train mà không thêm biến thể\n","\n","    transforms.ToTensor(),  # Chuyển ảnh thành tensor: mảng nhiều chiều\n","    transforms.Normalize(\n","        mean=[0.485, 0.456, 0.406],  # Trung bình RGB (ImageNet)\n","        std=[0.229, 0.224, 0.225]    # Độ lệch chuẩn RGB (ImageNet)\n","    )\n","])\n"]},{"cell_type":"markdown","metadata":{"id":"Lzu2fVZYNiIU"},"source":["3. TẠO DATA DATASET VÀ KIỂM TRA PHÂN BỐ LỚP"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"818zF62DNiIU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745124519050,"user_tz":-420,"elapsed":22799,"user":{"displayName":"Đức đẹp trai Anh","userId":"04958906416519110938"}},"outputId":"ef3fd6c4-afea-492c-df5e-372e3d5ca1b3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Phân bố lớp tập train ban đầu: Counter({0: 1000, 1: 1000, 2: 1000, 3: 1000, 4: 1000, 5: 1000, 6: 1000, 7: 1000, 8: 1000, 9: 1000, 10: 1000, 11: 1000, 12: 1000, 13: 1000, 14: 1000})\n","Phân bố lớp tập test: Counter({0: 200, 1: 200, 2: 200, 3: 200, 4: 200, 5: 200, 6: 200, 7: 200, 8: 200, 9: 200, 10: 200, 11: 200, 12: 200, 13: 200, 14: 200})\n","\n","Sau khi tách:\n","Phân bố lớp tập train: Counter({5: 800, 11: 800, 0: 800, 4: 800, 6: 800, 10: 800, 8: 800, 9: 800, 3: 800, 7: 800, 14: 800, 1: 800, 2: 800, 13: 800, 12: 800})\n","Phân bố lớp tập val: Counter({9: 200, 5: 200, 13: 200, 11: 200, 3: 200, 0: 200, 1: 200, 6: 200, 8: 200, 4: 200, 7: 200, 10: 200, 14: 200, 12: 200, 2: 200})\n"]}],"source":["# 1. Khởi tạo datasets với transform tương ứng\n","train_dataset = ActionDataset(\n","    '/content/drive/MyDrive/Colab Notebooks/Final/Data/train_data',\n","    transform=train_transform  # Áp dụng augment mạnh khi training\n",")\n","\n","test_dataset = ActionDataset(\n","    '/content/drive/MyDrive/Colab Notebooks/Final/Data/test_data',\n","    transform=test_transform  # Resize + crop cố định khi test\n",")\n","\n","# 2. Kiểm tra phân bố lớp ban đầu trong tập train và test\n","print(\"Phân bố lớp tập train ban đầu:\",\n","      Counter([label for _, label in train_dataset.images]))  # Đếm số lượng ảnh mỗi lớp trong train\n","\n","print(\"Phân bố lớp tập test:\",\n","      Counter([label for _, label in test_dataset.images]))   # Đếm số lượng ảnh mỗi lớp trong test\n","\n","# 3. Tách validation set từ train_dataset (tách 20% ảnh để làm tập val)\n","indices = list(range(len(train_dataset)))  # Danh sách chỉ số ảnh: [0, 1, 2, ..., N-1]\n","\n","# Sử dụng stratify để đảm bảo phân bố lớp giữa train/val là giống nhau\n","train_idx, val_idx = train_test_split(\n","    indices,\n","    test_size=0.2,               # 20% dùng làm validation\n","    random_state=42,             # Đảm bảo kết quả tách luôn giống nhau khi chạy lại\n","    stratify=[label for _, label in train_dataset.images]  # Giữ phân bố lớp\n",")\n","\n","# Tạo các tập con (subset) cho train và val\n","train_subset = torch.utils.data.Subset(train_dataset, train_idx)\n","val_subset = torch.utils.data.Subset(train_dataset, val_idx)\n","\n","# 4. Hàm hỗ trợ để đếm lại phân bố lớp trong các subset\n","def get_label_counts(subset):\n","    return Counter([subset.dataset.images[i][1] for i in subset.indices])\n","\n","# In ra phân bố lớp sau khi tách train/val\n","print(\"\\nSau khi tách:\")\n","print(\"Phân bố lớp tập train:\", get_label_counts(train_subset))\n","print(\"Phân bố lớp tập val:\", get_label_counts(val_subset))\n","\n","# 5. Gán lại transform cho val (dùng transform giống test để không làm méo ảnh)\n","# Vì val_subset vẫn dùng chung dataset gốc (train_dataset), nên cần cập nhật lại transform cho val\n","val_subset.dataset.transform = test_transform\n"]},{"cell_type":"markdown","source":["4. TẠO DATA LOADERS"],"metadata":{"id":"P5KwyRUX2r6H"}},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","\n","# 1. Tạo DataLoader cho tập train\n","train_loader = DataLoader(\n","    train_subset,       # Sử dụng tập con đã chia từ train_dataset\n","    batch_size=32,      # Số lượng ảnh xử lý mỗi batch\n","    shuffle=True,       # Trộn dữ liệu mỗi epoch để tránh overfitting\n","    num_workers=2,      # Số tiến trình phụ đọc dữ liệu song song (tùy máy)\n","    pin_memory=True,    # Giúp tăng tốc khi truyền dữ liệu sang GPU\n","    persistent_workers=False  # Có thể đặt True nếu bạn chạy nhiều epoch trong môi trường ổn định\n",")\n","\n","# 2. Tạo DataLoader cho tập validation\n","val_loader = DataLoader(\n","    val_subset,\n","    batch_size=32,\n","    shuffle=False,      # Không cần shuffle để giữ thứ tự kiểm định\n","    num_workers=2,\n","    pin_memory=True\n",")\n","\n","# 3. Tạo DataLoader cho tập test\n","test_loader = DataLoader(\n","    test_dataset,\n","    batch_size=32,\n","    shuffle=False,      # Không cần shuffle trong test\n","    num_workers=2,\n","    pin_memory=True\n",")\n"],"metadata":{"id":"oc1Y5wqu2vdN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GQrU1xbANiIU"},"source":["5. XÂY DỰNG MODEL VISION TRANSFORMER\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":309,"referenced_widgets":["8d28cd725f824bf89bb177d76ac9f918","fd47f142817a48d098a92bd630ae8dc1","7a947bdd172347bf81aecb757cb6b7f9","f8689be82c0e4e19a049c40ddf11797a","67a1701107c64875a164197a4777386c","6aee531250b344f1b8d1aee30126c086","58250af172304c688feb8116e4ec29ea","f247646b71ae48568ba66f4afb958315","5af442ac78a946219295f99047debf03","fcfdda8c46e14626a5cc8fc36df597c9","1642f9e22c5f4a5fa4da6d24eaaa6bf3","b68aaae8f3f1441da8c9b794add8dddf","62b4c48f3c174c4d831ef7b31d432d78","4076c70a3c0749d79df1fa53abe17c0b","588a0c5daa9f4172a8c47e15a8b0c9d8","5717922dc942493ebfc1c864870358c3","b34044897b82445ba0ddce1cf04842f0","1685a1c0527e48219dcd476ccefff1b1","971f3946b4f34844a86932871783315e","59491224b90348c3b2ebdc669453a141","e240b9c29da64300a27be965e708404d","846ae823884741c4ab36da0ff139e000"]},"executionInfo":{"elapsed":4288,"status":"ok","timestamp":1745124523344,"user":{"displayName":"Đức đẹp trai Anh","userId":"04958906416519110938"},"user_tz":-420},"id":"7D_hb1ceNiIU","outputId":"d24fa4f3-83d7-4b32-cfc6-163242973a9e"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/69.7k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d28cd725f824bf89bb177d76ac9f918"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"]},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b68aaae8f3f1441da8c9b794add8dddf"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n","- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([15]) in the model instantiated\n","- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([15, 768]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# Số lượng lớp đầu ra cần phân loại (phù hợp với bài toán của bạn)\n","num_classes = 15\n","\n","# 1. Tải cấu hình (config) của mô hình ViT gốc từ HuggingFace\n","config = ViTConfig.from_pretrained('google/vit-base-patch16-224')\n","\n","# 2. Cập nhật số lượng lớp đầu ra phù hợp với số lớp trong bài toán của bạn\n","config.num_labels = num_classes  # Cập nhật số lớp output classifier\n","\n","# 3. Cấu hình dropout để giảm overfitting (nếu dữ liệu nhỏ, nên tăng một chút)\n","config.hidden_dropout_prob = 0.3               # Dropout tại các lớp MLP (Feed-forward layers)\n","config.attention_probs_dropout_prob = 0.3      # Dropout tại attention layers\n","\n","# 4. Khởi tạo mô hình ViT với trọng số pretrained từ mô hình gốc\n","model = ViTForImageClassification.from_pretrained(\n","    'google/vit-base-patch16-224',  # Sử dụng mô hình ViT được pretrain trên ImageNet-21k\n","    config=config,                  # Áp dụng config đã chỉnh sửa phía trên\n","    ignore_mismatched_sizes=True    # Bỏ qua lỗi nếu kích thước lớp cuối (head) không khớp — do ta đổi số lớp\n",")\n"]},{"cell_type":"markdown","metadata":{"id":"r2TBfHYQNiIU"},"source":["6. HÀM HỖ TRỢ"]},{"cell_type":"code","source":["# Tính độ chính xác của 1 patch\n","def calculate_accuracy(y_pred, y_true): #y_pred: giá trị dự đoán, y_true: nhãn thật\n","    _, predicted = torch.max(y_pred, 1)  # Lấy chỉ số lớp có xác suất cao nhất\n","    correct = (predicted == y_true).sum().item()  # Đếm số lượng dự đoán đúng\n","    return correct / y_true.size(0)  # Tính độ chính xác (accuracy) = đúng / tổng\n","#Đánh giá mô hình trên tập Validation/test\n","def evaluate(model, dataloader):\n","    model.eval()  # Đưa mô hình về chế độ đánh giá (tắt dropout, batchnorm,...)\n","    total_loss, total_acc = 0.0, 0.0  # Biến lưu tổng loss và accuracy\n","    all_preds, all_labels = [], []   # Danh sách lưu toàn bộ dự đoán và nhãn thật\n","\n","    with torch.no_grad():  # Không tính gradient (giúp tiết kiệm bộ nhớ, tăng tốc)\n","        for images, labels in dataloader:\n","            images, labels = images.to(device), labels.to(device)  # Đưa dữ liệu lên GPU (nếu có)\n","            outputs = model(images)  # Dự đoán đầu ra qua mô hình\n","            loss = criterion(outputs.logits, labels)  # Tính loss giữa dự đoán và nhãn thật\n","\n","            total_loss += loss.item()  # Cộng dồn loss\n","            total_acc += calculate_accuracy(outputs.logits, labels)  # Tính và cộng dồn độ chính xác\n","\n","            # Lưu lại toàn bộ nhãn dự đoán và nhãn thật để đánh giá sau\n","            all_preds.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())  # Dự đoán lớp\n","            all_labels.extend(labels.cpu().numpy())  # Nhãn thật\n","\n","    # Trả về:\n","    # - Loss trung bình\n","    # - Accuracy trung bình\n","    # - Danh sách dự đoán tất cả ảnh\n","    # - Danh sách nhãn thật tất cả ảnh\n","    return (total_loss / len(dataloader),\n","            total_acc / len(dataloader),\n","            all_preds,\n","            all_labels)\n"],"metadata":{"id":"v9PJGBsP3C5P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["7. THIẾT LẬP TRAINNING"],"metadata":{"id":"PdPIDyYp3OmD"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"GPXAKgL4NiIU","executionInfo":{"status":"ok","timestamp":1745124523790,"user_tz":-420,"elapsed":361,"user":{"displayName":"Đức đẹp trai Anh","userId":"04958906416519110938"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ec5f5314-a8bc-4ff8-9be6-3c4a1d085320"},"outputs":[{"output_type":"stream","name":"stdout","text":["Đang sử dụng thiết bị: cuda\n"]}],"source":["# Chọn thiết bị tính toán (nếu có GPU thì dùng, không thì dùng CPU)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Đang sử dụng thiết bị: {device}\")\n","\n","# Đưa mô hình lên thiết bị (GPU hoặc CPU)\n","model = model.to(device)\n","\n","# Hàm mất mát (Loss function) dùng cho phân loại nhiều lớp\n","criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n","# Trình tối ưu hóa AdamW với learning rate nhỏ để fine-tune mô hình pretrained\n","optimizer = optim.AdamW(\n","    model.parameters(),    # Các tham số sẽ được tối ưu\n","    lr=3e-5,               # Learning rate nhỏ vì đang fine-tune mô hình đã được pretrained\n","    weight_decay=0.01      # Regularization (chuẩn hóa) để tránh overfitting\n",")\n","# Scheduler giúp điều chỉnh learning rate trong quá trình huấn luyện\n","scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n","    optimizer,\n","    mode='max',         # Theo dõi metric có xu hướng tăng (accuracy)\n","    patience=2,         # Nếu val_acc không tăng sau 2 epochs → giảm LR\n","    factor=0.5,         # Giảm learning rate còn 50%\n","    verbose=True        # In thông tin khi learning rate được giảm\n",")\n","\n","# Số epoch huấn luyện\n","num_epochs = 15\n","# Biến lưu kết quả tốt nhất để dùng cho Early Stopping\n","best_val_acc = 0.0       # Lưu best accuracy trên tập validation\n","patience = 3             # Dừng sớm nếu val_acc không cải thiện sau 3 epochs\n","no_improve = 0           # Đếm số epoch liên tiếp không cải thiện\n","\n","# Danh sách lưu lại loss và accuracy mỗi epoch (để vẽ biểu đồ sau)\n","train_losses, val_losses = [], []\n","train_accs, val_accs = [], []\n"]},{"cell_type":"markdown","source":["8. VÒNG LẶP TRAINING"],"metadata":{"id":"y3WdhZIt3kGu"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"MlJH5wbtNiIV","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3839afe3-5a43-4a0a-e6d0-8624983acc0d"},"outputs":[{"output_type":"stream","name":"stderr","text":["Epoch 1/15: 100%|██████████| 375/375 [59:17<00:00,  9.49s/it, loss=1.33, acc=0.688]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1/15:\n","Train Loss: 1.7350 | Train Acc: 0.5364\n","Val Loss: 1.1994 | Val Acc: 0.7735\n","Best model saved with val acc: 0.7735\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/15: 100%|██████████| 375/375 [07:03<00:00,  1.13s/it, loss=0.924, acc=0.875]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 2/15:\n","Train Loss: 1.2067 | Train Acc: 0.7473\n","Val Loss: 1.0576 | Val Acc: 0.8096\n","Best model saved with val acc: 0.8096\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/15: 100%|██████████| 375/375 [07:02<00:00,  1.13s/it, loss=0.868, acc=0.875]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 3/15:\n","Train Loss: 1.0709 | Train Acc: 0.8020\n","Val Loss: 1.0219 | Val Acc: 0.8254\n","Best model saved with val acc: 0.8254\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4/15: 100%|██████████| 375/375 [07:01<00:00,  1.12s/it, loss=0.75, acc=0.906]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 4/15:\n","Train Loss: 0.9873 | Train Acc: 0.8367\n","Val Loss: 1.0029 | Val Acc: 0.8300\n","Best model saved with val acc: 0.8300\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5/15: 100%|██████████| 375/375 [07:02<00:00,  1.13s/it, loss=0.761, acc=0.906]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 5/15:\n","Train Loss: 0.8880 | Train Acc: 0.8768\n","Val Loss: 0.9908 | Val Acc: 0.8370\n","Best model saved with val acc: 0.8370\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 6/15: 100%|██████████| 375/375 [07:03<00:00,  1.13s/it, loss=0.788, acc=0.938]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 6/15:\n","Train Loss: 0.8484 | Train Acc: 0.8929\n","Val Loss: 0.9957 | Val Acc: 0.8387\n","Best model saved with val acc: 0.8387\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 7/15: 100%|██████████| 375/375 [07:03<00:00,  1.13s/it, loss=0.694, acc=0.969]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 7/15:\n","Train Loss: 0.8191 | Train Acc: 0.9056\n","Val Loss: 0.9821 | Val Acc: 0.8383\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 8/15: 100%|██████████| 375/375 [06:43<00:00,  1.08s/it, loss=0.981, acc=0.844]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 8/15:\n","Train Loss: 0.7796 | Train Acc: 0.9217\n","Val Loss: 1.0095 | Val Acc: 0.8357\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 9/15: 100%|██████████| 375/375 [06:43<00:00,  1.08s/it, loss=0.719, acc=0.938]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 9/15:\n","Train Loss: 0.7644 | Train Acc: 0.9265\n","Val Loss: 1.0038 | Val Acc: 0.8340\n","No improvement for 3 epochs. Early stopping!\n"]}],"source":["for epoch in range(num_epochs):\n","    # ======= PHASE 1: HUẤN LUYỆN =========\n","    model.train()  # Đặt model vào chế độ huấn luyện (training mode)\n","    epoch_train_loss, epoch_train_acc = 0.0, 0.0  # Biến lưu tổng loss và acc của epoch\n","\n","    # Hiển thị progress bar với tqdm để dễ theo dõi tiến trình huấn luyện\n","    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n","    for images, labels in progress_bar:\n","        images, labels = images.to(device), labels.to(device)  # Đưa dữ liệu lên GPU nếu có\n","\n","        optimizer.zero_grad()  # Reset gradient trước mỗi batch\n","        outputs = model(images)  # Dự đoán đầu ra từ mô hình\n","        loss = criterion(outputs.logits, labels)  # Tính loss\n","        loss.backward()  # Lan truyền gradient ngược\n","\n","        # Cắt gradient để tránh exploding gradients (giữ ổn định khi fine-tuning)\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","\n","        optimizer.step()  # Cập nhật trọng số\n","\n","        # Tính độ chính xác và cộng dồn loss + acc cho cả epoch\n","        acc = calculate_accuracy(outputs.logits, labels)\n","        epoch_train_loss += loss.item()\n","        epoch_train_acc += acc\n","\n","        # Cập nhật hiển thị tiến trình với loss và acc hiện tại\n","        progress_bar.set_postfix({'loss': loss.item(), 'acc': acc})\n","\n","\n","    # Trung bình loss và acc của cả epoch trên tập train\n","    avg_train_loss = epoch_train_loss / len(train_loader)\n","    avg_train_acc = epoch_train_acc / len(train_loader)\n","\n","    # Đánh giá trên tập validation (tách riêng ở bước chuẩn bị dữ liệu)\n","    avg_val_loss, avg_val_acc, val_preds, val_labels = evaluate(model, val_loader)\n","\n","    # Cập nhật scheduler (giảm LR nếu val_loss không cải thiện)\n","    scheduler.step(avg_val_loss)\n","\n","    # Lưu lại loss và acc của epoch này\n","    train_losses.append(avg_train_loss)\n","    val_losses.append(avg_val_loss)\n","    train_accs.append(avg_train_acc)\n","    val_accs.append(avg_val_acc)\n","\n","\n","    # In thông tin sau mỗi epochs\n","    print(f'\\nEpoch {epoch+1}/{num_epochs}:')\n","    print(f'Train Loss: {avg_train_loss:.4f} | Train Acc: {avg_train_acc:.4f}')\n","    print(f'Val Loss: {avg_val_loss:.4f} | Val Acc: {avg_val_acc:.4f}')\n","\n","    # Early stopping và lưu model tốt nhất\n","    if avg_val_acc > best_val_acc:\n","        best_val_acc = avg_val_acc\n","        no_improve = 0 # Reset bộ đếm early stopping\n","        model_path = '/content/drive/MyDrive/Colab Notebooks/Final/best_vit_model.pth'\n","        torch.save({\n","            'model_state_dict': model.state_dict(),            # Trọng số mô hình\n","            'optimizer_state_dict': optimizer.state_dict(),    # Trạng thái optimizer (dùng nếu muốn tiếp tục train)\n","            'epoch': epoch,                                    # Epoch hiện tại\n","            'best_val_acc': best_val_acc                       # Độ chính xác val tốt nhất\n","        }, model_path)\n","        print(f'Best model saved with val acc: {best_val_acc:.4f}')\n","    else:\n","        no_improve += 1 # Tăng bộ đếm nếu không cải thiện\n","        if no_improve >= patience:\n","            print(f\"No improvement for {patience} epochs. Early stopping!\")\n","            break # Dừng huấn luyện sớm nếu không cải thiện trong vài epoch liên tiếp\n","\n","# Đánh giá cuối cùng trên test set\n","# Load lại mô hình có val acc cao nhất để đánh giá trên test\n","model.load_state_dict(torch.load(model_path)['model_state_dict'])\n","# Gọi hàm evaluate để tính test loss, accuracy và lưu lại các dự đoán\n","test_loss, test_acc, test_preds, test_labels = evaluate(model, test_loader)\n","# In ra độ chính xác cuối cùng và bảng phân loại\n","print(f'\\nFinal Test Accuracy: {test_acc:.4f}')\n","print('Classification Report:')\n","print(classification_report(test_labels, test_preds, target_names=train_dataset.classes))"]},{"cell_type":"markdown","source":["LOAD MODEL ĐÃ TRAIN\n"],"metadata":{"id":"GW21YNGDS1OH"}},{"cell_type":"code","source":["# 1. Khởi tạo device (CPU/GPU)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Đang sử dụng thiết bị: {device}\")\n","\n","# 2. Khởi tạo lại model với cùng cấu hình ban đầu\n","num_classes = 15\n","config = ViTConfig.from_pretrained('google/vit-base-patch16-224')\n","config.num_labels = num_classes\n","config.hidden_dropout_prob = 0.3\n","config.attention_probs_dropout_prob = 0.3\n","\n","model = ViTForImageClassification.from_pretrained(\n","    'google/vit-base-patch16-224',\n","    config=config,\n","    ignore_mismatched_sizes=True\n",").to(device)\n","\n","# 3. Load trọng số đã lưu\n","model_path = '/content/drive/MyDrive/Colab Notebooks/Final/best_vit_model.pth'\n","checkpoint = torch.load(model_path, map_location=device)\n","model.load_state_dict(checkpoint['model_state_dict'])\n","\n","# 4. Load optimizer state nếu cần (cho việc tiếp tục training)\n","# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","\n","# 5. Đưa model vào chế độ evaluation\n","model.eval()\n","\n","# 6. Kiểm tra model đã load\n","print(\"\\nModel đã được load thành công từ:\", model_path)\n","print(f\"Best validation accuracy khi train: {checkpoint['best_val_acc']:.4f}\")\n","print(f\"Epoch cuối cùng được train: {checkpoint['epoch'] + 1}\")\n","\n","# 7. Kiểm tra nhanh bằng cách in ra kiến trúc model\n","print(\"\\nKiến trúc model:\")\n","print(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3bjdEbADS0Mw","executionInfo":{"status":"ok","timestamp":1745389229578,"user_tz":-420,"elapsed":18985,"user":{"displayName":"Đức đẹp trai Anh","userId":"04958906416519110938"}},"outputId":"fbfd9f12-1261-40b6-f44b-0aa6bfaa5c83"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Đang sử dụng thiết bị: cuda\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n","- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([15]) in the model instantiated\n","- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([15, 768]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Model đã được load thành công từ: /content/drive/MyDrive/Colab Notebooks/Final/best_vit_model.pth\n","Best validation accuracy khi train: 0.8387\n","Epoch cuối cùng được train: 6\n","\n","Kiến trúc model:\n","ViTForImageClassification(\n","  (vit): ViTModel(\n","    (embeddings): ViTEmbeddings(\n","      (patch_embeddings): ViTPatchEmbeddings(\n","        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n","      )\n","      (dropout): Dropout(p=0.3, inplace=False)\n","    )\n","    (encoder): ViTEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x ViTLayer(\n","          (attention): ViTAttention(\n","            (attention): ViTSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","            )\n","            (output): ViTSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.3, inplace=False)\n","            )\n","          )\n","          (intermediate): ViTIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ViTOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.3, inplace=False)\n","          )\n","          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","      )\n","    )\n","    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","  )\n","  (classifier): Linear(in_features=768, out_features=15, bias=True)\n",")\n"]}]},{"cell_type":"markdown","source":["9. BIEU DO TRAIN, LOSS"],"metadata":{"id":"wys1yLYvYgH0"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"-ZUjrmN1NiIV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745389288092,"user_tz":-420,"elapsed":11,"user":{"displayName":"Đức đẹp trai Anh","userId":"04958906416519110938"}},"outputId":"72ae4e85-5363-4b12-fedf-ac38c294ee7d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Không tìm thấy training history trong checkpoint. Không thể vẽ đồ thị.\n"]}],"source":["import matplotlib.pyplot as plt\n","\n","# Kiểm tra nếu các biến đã tồn tại trong checkpoint\n","if 'train_losses' in checkpoint and 'val_losses' in checkpoint and 'train_accs' in checkpoint and 'val_accs' in checkpoint:\n","    train_losses = checkpoint['train_losses']\n","    val_losses = checkpoint['val_losses']\n","    train_accs = checkpoint['train_accs']\n","    val_accs = checkpoint['val_accs']\n","\n","    # Vẽ đồ thị\n","    plt.figure(figsize=(12, 5))\n","\n","    # Subplot 1: Loss\n","    plt.subplot(1, 2, 1)\n","    plt.plot(train_losses, label='Train Loss')\n","    plt.plot(val_losses, label='Val Loss')\n","    plt.title('Training and Validation Loss')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","\n","    # Subplot 2: Accuracy\n","    plt.subplot(1, 2, 2)\n","    plt.plot(train_accs, label='Train Acc')\n","    plt.plot(val_accs, label='Val Acc')\n","    plt.title('Training and Validation Accuracy')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Accuracy')\n","    plt.legend()\n","\n","    plt.tight_layout()\n","    plt.show()\n","else:\n","    print(\"Không tìm thấy training history trong checkpoint. Không thể vẽ đồ thị.\")"]},{"cell_type":"markdown","metadata":{"id":"7VPulLd_NiIV"},"source":["10. ĐÁNH GIÁ MÔ HÌNH\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dp274XM9NiIV"},"outputs":[],"source":["test_loss, test_acc, test_preds, test_labels = evaluate(model, test_loader)\n","print(f'\\nTest Accuracy: {test_acc:.4f}')\n","print('\\nClassification Report:')\n","print(classification_report(test_labels, test_preds, target_names=train_dataset.classes))"]},{"cell_type":"markdown","metadata":{"id":"ZUgnckc5NiIV"},"source":["11. CONFUSION MATRIX"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H7vNyBUfNiIV"},"outputs":[],"source":["cm = confusion_matrix(test_labels, test_preds)\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(cm, annot=True, fmt='d', xticklabels=train_dataset.classes, yticklabels=train_dataset.classes)\n","plt.title('Confusion Matrix')\n","plt.xlabel('Predicted')\n","plt.ylabel('True')\n","plt.show()"]},{"cell_type":"code","source":["from google.colab import runtime\n","runtime.save()\n","print(\"Đã lưu toàn bộ output vào notebook\")\n"],"metadata":{"id":"Pitw0ZEm5tle"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNkkE6ETPp20ukQr69M4Wyq"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"8d28cd725f824bf89bb177d76ac9f918":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fd47f142817a48d098a92bd630ae8dc1","IPY_MODEL_7a947bdd172347bf81aecb757cb6b7f9","IPY_MODEL_f8689be82c0e4e19a049c40ddf11797a"],"layout":"IPY_MODEL_67a1701107c64875a164197a4777386c"}},"fd47f142817a48d098a92bd630ae8dc1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6aee531250b344f1b8d1aee30126c086","placeholder":"​","style":"IPY_MODEL_58250af172304c688feb8116e4ec29ea","value":"config.json: 100%"}},"7a947bdd172347bf81aecb757cb6b7f9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f247646b71ae48568ba66f4afb958315","max":69665,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5af442ac78a946219295f99047debf03","value":69665}},"f8689be82c0e4e19a049c40ddf11797a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fcfdda8c46e14626a5cc8fc36df597c9","placeholder":"​","style":"IPY_MODEL_1642f9e22c5f4a5fa4da6d24eaaa6bf3","value":" 69.7k/69.7k [00:00&lt;00:00, 7.11MB/s]"}},"67a1701107c64875a164197a4777386c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6aee531250b344f1b8d1aee30126c086":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58250af172304c688feb8116e4ec29ea":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f247646b71ae48568ba66f4afb958315":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5af442ac78a946219295f99047debf03":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fcfdda8c46e14626a5cc8fc36df597c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1642f9e22c5f4a5fa4da6d24eaaa6bf3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b68aaae8f3f1441da8c9b794add8dddf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_62b4c48f3c174c4d831ef7b31d432d78","IPY_MODEL_4076c70a3c0749d79df1fa53abe17c0b","IPY_MODEL_588a0c5daa9f4172a8c47e15a8b0c9d8"],"layout":"IPY_MODEL_5717922dc942493ebfc1c864870358c3"}},"62b4c48f3c174c4d831ef7b31d432d78":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b34044897b82445ba0ddce1cf04842f0","placeholder":"​","style":"IPY_MODEL_1685a1c0527e48219dcd476ccefff1b1","value":"model.safetensors: 100%"}},"4076c70a3c0749d79df1fa53abe17c0b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_971f3946b4f34844a86932871783315e","max":346293852,"min":0,"orientation":"horizontal","style":"IPY_MODEL_59491224b90348c3b2ebdc669453a141","value":346293852}},"588a0c5daa9f4172a8c47e15a8b0c9d8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e240b9c29da64300a27be965e708404d","placeholder":"​","style":"IPY_MODEL_846ae823884741c4ab36da0ff139e000","value":" 346M/346M [00:00&lt;00:00, 373MB/s]"}},"5717922dc942493ebfc1c864870358c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b34044897b82445ba0ddce1cf04842f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1685a1c0527e48219dcd476ccefff1b1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"971f3946b4f34844a86932871783315e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"59491224b90348c3b2ebdc669453a141":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e240b9c29da64300a27be965e708404d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"846ae823884741c4ab36da0ff139e000":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}