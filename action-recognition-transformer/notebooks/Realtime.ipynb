{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1Jvh2pkNowr0xNWnu7Bzu42N9kmLtE-S-","authorship_tag":"ABX9TyPd4sSO2FwOJZ4wJ5zp8buw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"f862c73594c9492fa56f8608e8efd0f9":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_f6fcb5e4224a46c0a5d807d68b41289e","IPY_MODEL_4b179edf52404e119a4109a5d30d1f46","IPY_MODEL_25857fafccb74352b2652d01b5b53147"],"layout":"IPY_MODEL_56f695ea2ea24498b8170eba670522dd"}},"f6fcb5e4224a46c0a5d807d68b41289e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8773b3a258444b47a4dd831666bbcae4","IPY_MODEL_e8fd0112338c42bba1353aeaab897a0d"],"layout":"IPY_MODEL_21b7b44c996d4317a6444337a83c89dc"}},"4b179edf52404e119a4109a5d30d1f46":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"IntProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"Ti·∫øn ƒë·ªô:","description_tooltip":null,"layout":"IPY_MODEL_ee6ab2a999a04a0999e00c56420158a2","max":100,"min":0,"orientation":"horizontal","style":"IPY_MODEL_794d41aa5a8e493aa68c03dcb712bda9","value":0}},"25857fafccb74352b2652d01b5b53147":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7c9b2f0e2e0d44ada8847277255ba515","placeholder":"‚Äã","style":"IPY_MODEL_6d4b8e194e9d4c5da4a2cbfe9600b88e","value":"üõë ƒê√£ d·ª´ng x·ª≠ l√Ω"}},"56f695ea2ea24498b8170eba670522dd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8773b3a258444b47a4dd831666bbcae4":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"success","description":"B·∫¨T CAMERA","disabled":false,"icon":"video","layout":"IPY_MODEL_731a0cbb2bb1468c9e97381bccd7428c","style":"IPY_MODEL_aa5f9c172d214b7187ac724feb5f2586","tooltip":""}},"e8fd0112338c42bba1353aeaab897a0d":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"danger","description":"T·∫ÆT CAMERA","disabled":false,"icon":"stop","layout":"IPY_MODEL_63e23d0ca89549d9b6255cac0649b037","style":"IPY_MODEL_394d5ff539a54b9ea1ea0f9690341537","tooltip":""}},"21b7b44c996d4317a6444337a83c89dc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee6ab2a999a04a0999e00c56420158a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"794d41aa5a8e493aa68c03dcb712bda9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":"#4CAF50","description_width":""}},"7c9b2f0e2e0d44ada8847277255ba515":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d4b8e194e9d4c5da4a2cbfe9600b88e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"731a0cbb2bb1468c9e97381bccd7428c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":"45px","justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"150px"}},"aa5f9c172d214b7187ac724feb5f2586":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"63e23d0ca89549d9b6255cac0649b037":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":"45px","justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"150px"}},"394d5ff539a54b9ea1ea0f9690341537":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":156},"id":"a-oGZTj5nyip","executionInfo":{"status":"ok","timestamp":1744656769532,"user_tz":-420,"elapsed":291292,"user":{"displayName":"ƒê·ª©c ƒë·∫πp trai Anh","userId":"04958906416519110938"}},"outputId":"0ac761e2-6cee-4427-e50b-11579a91b1cd"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m49.6/49.6 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m586.0/586.0 MB\u001b[0m \u001b[31m651.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["# [1] D·ªçn d·∫πp m√¥i tr∆∞·ªùng c≈©\n","!pip uninstall -y opencv-python opencv-python-headless numpy torch tensorflow > /dev/null 2>&1\n","!rm -rf ~/.cache/pip\n","\n","# [2] C√†i ƒë·∫∑t phi√™n b·∫£n chu·∫©n (d√πng --no-deps ƒë·ªÉ tr√°nh xung ƒë·ªôt)\n","!pip install --no-deps -q numpy==1.24.4\n","!pip install --no-deps -q opencv-python-headless==4.9.0.80\n","!pip install --no-deps -q torch==2.0.1+cu118 torchvision==0.15.2+cu118 --index-url https://download.pytorch.org/whl/cu118\n","!pip install --no-deps -q tensorflow==2.12.0  # Phi√™n b·∫£n ·ªïn ƒë·ªãnh v·ªõi Colab\n","!pip install --no-deps -q protobuf==3.20.3  # Tr√°nh xung ƒë·ªôt v·ªõi TensorFlow\n","!pip install --no-deps -q transformers==4.30.0\n","!pip install --no-deps -q protobuf==3.20.3\n","\n","# [3] Ch·ªß ƒë·ªông kh·ªüi ƒë·ªông l·∫°i b·∫±ng Python\n","from google.colab import runtime\n","runtime.unassign()"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"jXGQRB07pgiB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744741625463,"user_tz":-420,"elapsed":3528,"user":{"displayName":"ƒê·ª©c ƒë·∫πp trai Anh","userId":"04958906416519110938"}},"outputId":"d55e36a9-949e-4b59-f1c4-625b46922c14"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["YOLO_CFG = \"/content/drive/MyDrive/Colab Notebooks/Final/yolov3-tiny.cfg\"\n","YOLO_WEIGHTS = \"/content/drive/MyDrive/Colab Notebooks/Final/yolov3-tiny.weights\"\n","VIT_MODEL_PATH = \"/content/drive/MyDrive/Colab Notebooks/Final/best_vit_model.pth\"\n","MOVENET_PATH = \"/content/drive/MyDrive/Colab Notebooks/Final/movenet_lightning.tflite\"\n"],"metadata":{"id":"Po1gnQTpVLJ4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","import torch\n","import torch.nn.functional as F\n","from PIL import Image\n","import tensorflow as tf\n","import warnings\n","from IPython.display import display, Javascript, HTML\n","import ipywidgets as widgets\n","import time\n","import base64\n","import threading\n","import os\n","import traceback\n","from transformers import ViTConfig, ViTForImageClassification\n","\n","warnings.filterwarnings('ignore')\n","\n","# ==================== C·∫§U H√åNH ====================\n","YOLO_CFG = \"/content/drive/MyDrive/Colab Notebooks/Final/yolov3-tiny.cfg\"\n","YOLO_WEIGHTS = \"/content/drive/MyDrive/Colab Notebooks/Final/yolov3-tiny.weights\"\n","VIT_MODEL_PATH = \"/content/drive/MyDrive/Colab Notebooks/Final/best_vit_model.pth\"\n","MOVENET_PATH = \"/content/drive/MyDrive/Colab Notebooks/Final/movenet_lightning.tflite\"\n","\n","CLASS_NAMES = [\n","    \"calling\", \"clapping\", \"cycling\", \"dancing\", \"drinking\",\n","    \"eating\", \"fighting\", \"hugging\", \"laughing\", \"listening_to_music\",\n","    \"running\", \"sitting\", \"sleeping\", \"texting\", \"using_laptop\"\n","]\n","\n","SKELETON_CONNECTIONS = [\n","    # ƒê·∫ßu v√† c·ªï\n","    (5, 6),    # Tai tr√°i - Tai ph·∫£i\n","    (5, 0),    # Tai tr√°i - M≈©i\n","    (6, 0),    # Tai ph·∫£i - M≈©i\n","    (0, 1),    # M≈©i - M·∫Øt tr√°i\n","    (0, 2),    # M≈©i - M·∫Øt ph·∫£i\n","\n","    # Th√¢n tr√™n\n","    (5, 7),    # Tai tr√°i - Vai tr√°i\n","    (6, 8),    # Tai ph·∫£i - Vai ph·∫£i\n","    (7, 9),    # Vai tr√°i - Khu·ª∑u tay tr√°i\n","    (8, 10),   # Vai ph·∫£i - Khu·ª∑u tay ph·∫£i\n","    (7, 11),   # Vai tr√°i - H√¥ng tr√°i\n","    (8, 12),   # Vai ph·∫£i - H√¥ng ph·∫£i\n","    (11, 12),  # H√¥ng tr√°i - H√¥ng ph·∫£i\n","\n","    # Th√¢n d∆∞·ªõi\n","    (11, 13),  # H√¥ng tr√°i - ƒê·∫ßu g·ªëi tr√°i\n","    (12, 14),  # H√¥ng ph·∫£i - ƒê·∫ßu g·ªëi ph·∫£i\n","    (13, 15),  # ƒê·∫ßu g·ªëi tr√°i - M·∫Øt c√° tr√°i\n","    (14, 16),  # ƒê·∫ßu g·ªëi ph·∫£i - M·∫Øt c√° ph·∫£i\n","\n","    # Tay\n","    (9, 11),   # Khu·ª∑u tay tr√°i - C·ªï tay tr√°i\n","    (10, 12),  # Khu·ª∑u tay ph·∫£i - C·ªï tay ph·∫£i\n","]\n","\n","# ==================== KH·ªûI T·∫†O MODEL ====================\n","def init_models():\n","    print(\"üîß ƒêang kh·ªüi t·∫°o model...\")\n","    models = {}\n","\n","    try:\n","        # YOLO\n","        net = cv2.dnn.readNetFromDarknet(YOLO_CFG, YOLO_WEIGHTS)\n","        net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n","        net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n","        ln = net.getLayerNames()\n","        ln = [ln[i - 1] for i in net.getUnconnectedOutLayers()]\n","        models['yolo'] = (net, ln)\n","        print(\"‚úÖ YOLO loaded\")\n","\n","        # MoveNet\n","        interpreter = tf.lite.Interpreter(model_path=MOVENET_PATH)\n","        interpreter.allocate_tensors()\n","        models['movenet'] = interpreter\n","        print(\"‚úÖ MoveNet loaded\")\n","\n","    except Exception as e:\n","        print(f\"‚ùå L·ªói kh·ªüi t·∫°o model: {e}\")\n","        raise\n","\n","    return models\n","\n","# ==================== CAMERA ====================\n","class VideoStream:\n","    def __init__(self):\n","        self.is_active = False\n","        self.frame = None\n","        self.lock = threading.Lock()\n","        self.resolution = (640, 480)\n","        self.output_handle = None\n","        self.last_update = 0\n","\n","    def start(self):\n","        if not self.is_active:\n","            self.is_active = True\n","            # T·∫°o output widget ƒë·ªÉ hi·ªÉn th·ªã\n","            self.output_handle = display(display_id='video_output')\n","            print(f\"üì∑ Camera ƒë√£ b·∫≠t ({self.resolution[0]}x{self.resolution[1]})\")\n","\n","            js = Javascript(f\"\"\"\n","            const video = document.createElement('video');\n","            video.width = {self.resolution[0]};\n","            video.height = {self.resolution[1]};\n","            video.style.transform = 'scaleX(-1)';\n","            document.body.appendChild(video);\n","\n","            let lastFrameTime = 0;\n","\n","            const processFrame = async () => {{\n","                if (!window.cameraActive) return;\n","\n","                // Gi·ªõi h·∫°n t·ªëc ƒë·ªô frame (~20fps)\n","                const now = Date.now();\n","                if (now - lastFrameTime < 50) {{\n","                    requestAnimationFrame(processFrame);\n","                    return;\n","                }}\n","                lastFrameTime = now;\n","\n","                try {{\n","                    const canvas = document.createElement('canvas');\n","                    canvas.width = video.videoWidth;\n","                    canvas.height = video.videoHeight;\n","                    const ctx = canvas.getContext('2d');\n","                    ctx.drawImage(video, 0, 0);\n","\n","                    const imgData = canvas.toDataURL('image/jpeg', 0.8);\n","                    if (imgData && imgData.length > 1000) {{\n","                        google.colab.kernel.invokeFunction('notebook.processFrame', [imgData], {{}});\n","                    }}\n","                }} catch (e) {{\n","                    console.log(e);\n","                }}\n","\n","                requestAnimationFrame(processFrame);\n","            }};\n","\n","            navigator.mediaDevices.getUserMedia({{\n","                video: {{\n","                    width: {{ideal: {self.resolution[0]}}},\n","                    height: {{ideal: {self.resolution[1]}}},\n","                    facingMode: 'user',\n","                    frameRate: {{ideal: 30}}\n","                }}\n","            }}).then(stream => {{\n","                window.cameraActive = true;\n","                video.srcObject = stream;\n","                video.play();\n","                processFrame();\n","            }});\n","            \"\"\")\n","            display(js)\n","\n","    def stop(self):\n","        if self.is_active:\n","            self.is_active = False\n","            js = Javascript(\"\"\"\n","            window.cameraActive = false;\n","            const video = document.querySelector('video');\n","            if (video && video.srcObject) {\n","                video.srcObject.getTracks().forEach(track => track.stop());\n","                video.remove();\n","            }\n","            \"\"\")\n","            display(js)\n","            if self.output_handle:\n","                self.output_handle.update(HTML(\"\"))\n","            print(\"üõë Camera ƒë√£ t·∫Øt\")\n","\n","# ==================== ·ª®NG D·ª§NG CH√çNH ====================\n","class ActionRecognitionApp:\n","    def __init__(self):\n","        # Kh·ªüi t·∫°o thu·ªôc t√≠nh\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        print(f\"‚öôÔ∏è Thi·∫øt b·ªã: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n","\n","        # Kh·ªüi t·∫°o c√°c model\n","        self.models = init_models()\n","        self.stream = VideoStream()\n","        self.results = []\n","        self.fps = 0\n","        self.last_time = time.time()\n","        self.vit_ready = False\n","\n","        # Setup UI\n","        self.setup_ui()\n","\n","        # Load ViT model\n","        self.load_vit_model()\n","\n","    def setup_ui(self):\n","        \"\"\"Giao di·ªán ng∆∞·ªùi d√πng\"\"\"\n","        self.btn_start = widgets.Button(\n","            description=\"B·∫¨T CAMERA\",\n","            button_style='success',\n","            icon='video',\n","            layout=widgets.Layout(width='150px', height='45px'),\n","            disabled=True\n","        )\n","        self.btn_stop = widgets.Button(\n","            description=\"T·∫ÆT CAMERA\",\n","            button_style='danger',\n","            icon='stop',\n","            layout=widgets.Layout(width='150px', height='45px'),\n","            disabled=True\n","        )\n","\n","        self.progress = widgets.IntProgress(\n","            value=0,\n","            min=0,\n","            max=100,\n","            description='Ti·∫øn ƒë·ªô:',\n","            style={'bar_color': '#4CAF50'}\n","        )\n","\n","        self.status_label = widgets.Label(\n","            value=\"üîÑ ƒêang kh·ªüi t·∫°o h·ªá th·ªëng...\",\n","            style={'font_weight': 'bold'}\n","        )\n","\n","        self.btn_start.on_click(self._start_processing)\n","        self.btn_stop.on_click(self._stop_processing)\n","\n","        display(widgets.VBox([\n","            widgets.HBox([self.btn_start, self.btn_stop]),\n","            self.progress,\n","            self.status_label\n","        ]))\n","\n","    def update_status(self, message, progress=None):\n","        \"\"\"C·∫≠p nh·∫≠t tr·∫°ng th√°i\"\"\"\n","        self.status_label.value = message\n","        if progress is not None:\n","            self.progress.value = progress\n","        print(f\"[{time.strftime('%H:%M:%S')}] {message}\")\n","\n","    def _start_processing(self, b):\n","        \"\"\"B·∫Øt ƒë·∫ßu x·ª≠ l√Ω frame\"\"\"\n","        if self.vit_ready:\n","            self.stream.start()\n","            threading.Thread(target=self.process_frames, daemon=True).start()\n","            self.update_status(\"üìπ ƒêang x·ª≠ l√Ω video...\", 50)\n","        else:\n","            self.update_status(\"‚ö† Vui l√≤ng ch·ªù model t·∫£i xong!\", 0)\n","\n","    def _stop_processing(self, b):\n","        \"\"\"D·ª´ng x·ª≠ l√Ω\"\"\"\n","        self.stream.stop()\n","        self.update_status(\"üõë ƒê√£ d·ª´ng x·ª≠ l√Ω\", 0)\n","\n","    def load_vit_model(self):\n","        \"\"\"Load ViT model t·ª´ file ƒë√£ train\"\"\"\n","        try:\n","            self.update_status(\"‚ö° ƒêang t·∫£i ViT model...\", 30)\n","\n","            if not os.path.exists(VIT_MODEL_PATH):\n","                raise FileNotFoundError(f\"Kh√¥ng t√¨m th·∫•y file {VIT_MODEL_PATH}\")\n","\n","            config = ViTConfig.from_pretrained('google/vit-base-patch16-224')\n","            config.num_labels = len(CLASS_NAMES)\n","\n","            self.vit_model = ViTForImageClassification.from_pretrained(\n","                'google/vit-base-patch16-224',\n","                config=config,\n","                ignore_mismatched_sizes=True\n","            ).to(self.device)\n","\n","            state_dict = torch.load(VIT_MODEL_PATH, map_location=self.device)\n","\n","            if isinstance(state_dict, dict) and 'state_dict' in state_dict:\n","                self.vit_model.load_state_dict(state_dict['state_dict'])\n","            else:\n","                self.vit_model.load_state_dict(state_dict)\n","\n","            self.vit_model.eval()\n","\n","            self.processor = {\n","                'mean': [0.5, 0.5, 0.5],\n","                'std': [0.5, 0.5, 0.5],\n","                'size': {'height': 224, 'width': 224}\n","            }\n","\n","            test_input = torch.randn(1, 3, 224, 224).to(self.device)\n","            with torch.no_grad():\n","                output = self.vit_model(test_input).logits\n","            print(f\"‚úÖ Ki·ªÉm tra model th√†nh c√¥ng! Output shape: {output.shape}\")\n","\n","            self.vit_ready = True\n","            self.update_status(\"üéØ Model ƒë√£ s·∫µn s√†ng!\", 100)\n","            self.btn_start.disabled = False\n","            self.btn_stop.disabled = False\n","\n","        except Exception as e:\n","            self.handle_error(e)\n","\n","    def handle_error(self, error):\n","        \"\"\"X·ª≠ l√Ω l·ªói\"\"\"\n","        error_msg = str(error)[:200]\n","        self.update_status(f\"‚ùå L·ªói: {error_msg}\", 0)\n","        self.progress.bar_style = 'danger'\n","        traceback.print_exc()\n","\n","    def process_frames(self):\n","        \"\"\"X·ª≠ l√Ω t·ª´ng frame\"\"\"\n","        while self.stream.is_active:\n","            try:\n","                with self.stream.lock:\n","                    if self.stream.frame is None or self.stream.frame.size == 0:\n","                        time.sleep(0.01)\n","                        continue\n","\n","                    frame = self.stream.frame.copy()\n","\n","                # T√≠nh FPS\n","                now = time.time()\n","                self.fps = 0.9 * self.fps + 0.1 / (now - self.last_time) if (now - self.last_time) > 0 else 0\n","                self.last_time = now\n","\n","                # Ph√°t hi·ªán ng∆∞·ªùi\n","                boxes = self.detect_people(frame)\n","                self.results = []\n","\n","                # X·ª≠ l√Ω t·ª´ng ng∆∞·ªùi\n","                for (x, y, w, h) in boxes[:1]:\n","                    try:\n","                        # Nh·∫≠n di·ªán khung x∆∞∆°ng\n","                        keypoints = self.get_pose(frame, (x, y, w, h))\n","                        if np.mean(keypoints[:, 2]) > 0.2:\n","                            roi = frame[y:y+h, x:x+w]\n","                            if roi.size > 0:\n","                                action, conf = self.predict_action(Image.fromarray(roi))\n","                                self.results.append({\n","                                    'bbox': (x, y, w, h),\n","                                    'action': action,\n","                                    'confidence': conf,\n","                                    'keypoints': keypoints\n","                                })\n","                    except Exception as e:\n","                        print(f\"‚ö† L·ªói x·ª≠ l√Ω ng∆∞·ªùi: {e}\")\n","                        continue\n","\n","                # Hi·ªÉn th·ªã k·∫øt qu·∫£\n","                self.display_results(frame)\n","\n","            except Exception as e:\n","                print(f\"‚ö† L·ªói x·ª≠ l√Ω frame: {e}\")\n","                time.sleep(0.001)\n","\n","    def detect_people(self, image):\n","        \"\"\"Ph√°t hi·ªán ng∆∞·ªùi d√πng YOLO\"\"\"\n","        try:\n","            (H, W) = image.shape[:2]\n","            blob = cv2.dnn.blobFromImage(image, 1/255.0, (320, 320), swapRB=True, crop=False)\n","\n","            net, ln = self.models['yolo']\n","            net.setInput(blob)\n","            outputs = net.forward(ln)\n","\n","            boxes = []\n","            confidences = []\n","\n","            for output in outputs:\n","                for detection in output:\n","                    scores = detection[5:]\n","                    class_id = np.argmax(scores)\n","                    confidence = scores[class_id]\n","\n","                    if class_id == 0 and confidence > 0.5:\n","                        box = detection[0:4] * np.array([W, H, W, H])\n","                        (cx, cy, width, height) = box.astype(\"int\")\n","                        x = int(cx - (width / 2))\n","                        y = int(cy - (height / 2))\n","                        boxes.append([x, y, int(width), int(height)])\n","                        confidences.append(float(confidence))\n","\n","            indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n","            return [boxes[i] for i in indices] if len(indices) > 0 else []\n","\n","        except Exception as e:\n","            print(f\"‚ö† L·ªói ph√°t hi·ªán ng∆∞·ªùi: {e}\")\n","            return []\n","\n","    def get_pose(self, image, bbox):\n","        \"\"\"Nh·∫≠n di·ªán khung x∆∞∆°ng\"\"\"\n","        try:\n","            x, y, w, h = [int(v) for v in bbox]\n","            roi = image[y:y+h, x:x+w]\n","\n","            if roi.size == 0:\n","                return np.zeros((17, 3))\n","\n","            input_img = cv2.resize(roi, (192, 192))\n","            input_img = np.expand_dims(input_img, axis=0).astype(np.uint8)\n","\n","            interpreter = self.models['movenet']\n","            interpreter.set_tensor(interpreter.get_input_details()[0]['index'], input_img)\n","            interpreter.invoke()\n","            keypoints = interpreter.get_tensor(interpreter.get_output_details()[0]['index']).copy()[0,0]\n","\n","            keypoints[:, 0] = x + keypoints[:, 0] * w\n","            keypoints[:, 1] = y + keypoints[:, 1] * h\n","\n","            return keypoints\n","\n","        except Exception as e:\n","            print(f\"‚ö† L·ªói khung x∆∞∆°ng: {e}\")\n","            return np.zeros((17, 3))\n","\n","    def predict_action(self, image):\n","        \"\"\"D·ª± ƒëo√°n h√†nh ƒë·ªông\"\"\"\n","        try:\n","            # Ti·ªÅn x·ª≠ l√Ω ·∫£nh\n","            img = image.resize((224, 224))\n","            img_array = np.array(img) / 255.0\n","            img_array = (img_array - np.array(self.processor['mean'])) / np.array(self.processor['std'])\n","            img_tensor = torch.tensor(img_array, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0).to(self.device)\n","\n","            # D·ª± ƒëo√°n\n","            with torch.no_grad():\n","                outputs = self.vit_model(img_tensor).logits\n","                probs = F.softmax(outputs, dim=1)\n","                conf, pred = torch.max(probs, 1)\n","\n","            return CLASS_NAMES[pred.item()], conf.item()\n","\n","        except Exception as e:\n","            print(f\"‚ö† L·ªói d·ª± ƒëo√°n: {e}\")\n","            return \"unknown\", 0.0\n","\n","    def display_results(self, image):\n","        \"\"\"Hi·ªÉn th·ªã k·∫øt qu·∫£ real-time v·ªõi c√°c ƒëi·ªÉm kh·ªõp ch√≠nh x√°c\"\"\"\n","        try:\n","            output = image.copy()\n","\n","            # V·∫Ω FPS\n","            cv2.putText(output, f\"FPS: {self.fps:.1f}\", (10, 30),\n","                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n","\n","            # V·∫Ω t·ª´ng ng∆∞·ªùi\n","            for person in self.results:\n","                x, y, w, h = person['bbox']\n","                keypoints = person['keypoints']\n","\n","                # Bounding box\n","                cv2.rectangle(output, (x, y), (x+w, y+h), (0, 255, 0), 2)\n","\n","                # V·∫Ω khung x∆∞∆°ng v·ªõi m√†u s·∫Øc kh√°c nhau cho t·ª´ng ph·∫ßn\n","                for (i, j) in SKELETON_CONNECTIONS:\n","                    if (i < len(keypoints)) and (j < len(keypoints)) and (keypoints[i][2] > 0.2) and (keypoints[j][2] > 0.2):\n","                        # Ph√¢n lo·∫°i m√†u theo lo·∫°i kh·ªõp n·ªëi\n","                        if i in [5,6,0,1,2]:  # ƒê·∫ßu\n","                            color = (255, 192, 203)  # M√†u h·ªìng\n","                        elif i in [7,8,9,10]:  # Tay\n","                            color = (0, 255, 255)  # M√†u v√†ng\n","                        elif i in [11,12,13,14,15,16]:  # Ch√¢n\n","                            color = (255, 0, 255)  # M√†u t√≠m\n","                        else:  # Th√¢n\n","                            color = (0, 255, 0)  # M√†u xanh l√°\n","\n","                        cv2.line(output,\n","                                (int(keypoints[i][0]), int(keypoints[i][1])),\n","                                (int(keypoints[j][0]), int(keypoints[j][1])),\n","                                color, 3)\n","\n","                # V·∫Ω ƒëi·ªÉm kh·ªõp\n","                for i, kp in enumerate(keypoints):\n","                    if kp[2] > 0.2:\n","                        # M√†u s·∫Øc kh√°c nhau cho c√°c lo·∫°i kh·ªõp\n","                        if i in [5,6,0,1,2]:  # ƒê·∫ßu\n","                            color = (255, 0, 0)  # ƒê·ªè\n","                        elif i in [7,8,9,10]:  # Tay\n","                            color = (0, 0, 255)  # Xanh d∆∞∆°ng\n","                        elif i in [11,12,13,14,15,16]:  # Ch√¢n\n","                            color = (0, 255, 0)  # Xanh l√°\n","                        else:  # Th√¢n\n","                            color = (255, 255, 0)  # V√†ng\n","\n","                        cv2.circle(output, (int(kp[0]), int(kp[1])), 6, color, -1)\n","\n","                # Nh√£n h√†nh ƒë·ªông\n","                label = f\"{person['action']} ({person['confidence']:.0%})\"\n","                cv2.putText(output, label, (x, y-10),\n","                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n","\n","            # Hi·ªÉn th·ªã real-time\n","            if self.stream.output_handle and time.time() - self.stream.last_update > 0.033:\n","                self.stream.output_handle.update(Image.fromarray(output))\n","                self.stream.last_update = time.time()\n","\n","        except Exception as e:\n","            print(f\"‚ö† L·ªói hi·ªÉn th·ªã: {e}\")\n","\n","    def update_frame(self, frame_data):\n","        \"\"\"C·∫≠p nh·∫≠t frame t·ª´ JS\"\"\"\n","        try:\n","            if not frame_data or ',' not in frame_data:\n","                return\n","\n","            # Ki·ªÉm tra header base64\n","            header, encoded = frame_data.split(',', 1)\n","            if 'image' not in header:\n","                return\n","\n","            img_data = base64.b64decode(encoded)\n","            if len(img_data) == 0:\n","                return\n","\n","            img_array = np.frombuffer(img_data, np.uint8)\n","            if img_array.size == 0:\n","                return\n","\n","            frame = cv2.imdecode(img_array, cv2.IMREAD_COLOR)\n","            if frame is not None:\n","                with self.stream.lock:\n","                    self.stream.frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","        except Exception as e:\n","            pass\n","\n","# ==================== CH·∫†Y ·ª®NG D·ª§NG ====================\n","if __name__ == \"__main__\":\n","    # X√≥a cache\n","    torch.cuda.empty_cache()\n","\n","    # Ki·ªÉm tra h·ªá th·ªëng\n","    print(\"=\"*50)\n","    print(f\"PyTorch {torch.__version__}\")\n","    print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'Kh√¥ng c√≥'}\")\n","    print(\"=\"*50)\n","\n","    # Kh·ªüi ch·∫°y\n","    from google.colab import output\n","    app = ActionRecognitionApp()\n","    output.register_callback('notebook.processFrame', app.update_frame)\n","\n","    print(\"\"\"\n","    üöÄ ·ª®NG D·ª§NG NH·∫¨N DI·ªÜN H√ÄNH ƒê·ªòNG\n","    H∆Ø·ªöNG D·∫™N:\n","    1. Ch·ªù thanh ti·∫øn tr√¨nh ƒë·∫°t 100%\n","    2. Nh·∫•n [B·∫¨T CAMERA] ƒë·ªÉ b·∫Øt ƒë·∫ßu\n","    3. Nh·∫•n [T·∫ÆT CAMERA] ƒë·ªÉ d·ª´ng\n","    \"\"\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":601,"referenced_widgets":["f862c73594c9492fa56f8608e8efd0f9","f6fcb5e4224a46c0a5d807d68b41289e","4b179edf52404e119a4109a5d30d1f46","25857fafccb74352b2652d01b5b53147","56f695ea2ea24498b8170eba670522dd","8773b3a258444b47a4dd831666bbcae4","e8fd0112338c42bba1353aeaab897a0d","21b7b44c996d4317a6444337a83c89dc","ee6ab2a999a04a0999e00c56420158a2","794d41aa5a8e493aa68c03dcb712bda9","7c9b2f0e2e0d44ada8847277255ba515","6d4b8e194e9d4c5da4a2cbfe9600b88e","731a0cbb2bb1468c9e97381bccd7428c","aa5f9c172d214b7187ac724feb5f2586","63e23d0ca89549d9b6255cac0649b037","394d5ff539a54b9ea1ea0f9690341537"]},"id":"3dbOBYoCKgKk","outputId":"ec4c2f4d-b0f9-40ce-a502-41d00ca2b8ba","executionInfo":{"status":"ok","timestamp":1744745652880,"user_tz":-420,"elapsed":1272,"user":{"displayName":"ƒê·ª©c ƒë·∫πp trai Anh","userId":"04958906416519110938"}}},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["==================================================\n","PyTorch 2.6.0+cu124\n","GPU: Tesla T4\n","==================================================\n","‚öôÔ∏è Thi·∫øt b·ªã: Tesla T4\n","üîß ƒêang kh·ªüi t·∫°o model...\n","‚úÖ YOLO loaded\n","‚úÖ MoveNet loaded\n"]},{"output_type":"display_data","data":{"text/plain":["VBox(children=(HBox(children=(Button(button_style='success', description='B·∫¨T CAMERA', disabled=True, icon='vi‚Ä¶"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f862c73594c9492fa56f8608e8efd0f9"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[19:34:11] ‚ö° ƒêang t·∫£i ViT model...\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n","- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([15]) in the model instantiated\n","- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([15, 768]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["‚úÖ Ki·ªÉm tra model th√†nh c√¥ng! Output shape: torch.Size([1, 15])\n","[19:34:13] üéØ Model ƒë√£ s·∫µn s√†ng!\n","\n","    üöÄ ·ª®NG D·ª§NG NH·∫¨N DI·ªÜN H√ÄNH ƒê·ªòNG\n","    H∆Ø·ªöNG D·∫™N:\n","    1. Ch·ªù thanh ti·∫øn tr√¨nh ƒë·∫°t 100%\n","    2. Nh·∫•n [B·∫¨T CAMERA] ƒë·ªÉ b·∫Øt ƒë·∫ßu\n","    3. Nh·∫•n [T·∫ÆT CAMERA] ƒë·ªÉ d·ª´ng\n","    \n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["üì∑ Camera ƒë√£ b·∫≠t (640x480)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","            const video = document.createElement('video');\n","            video.width = 640;\n","            video.height = 480;\n","            video.style.transform = 'scaleX(-1)';\n","            document.body.appendChild(video);\n","\n","            let lastFrameTime = 0;\n","\n","            const processFrame = async () => {\n","                if (!window.cameraActive) return;\n","                \n","                // Gi·ªõi h·∫°n t·ªëc ƒë·ªô frame (~20fps)\n","                const now = Date.now();\n","                if (now - lastFrameTime < 50) {\n","                    requestAnimationFrame(processFrame);\n","                    return;\n","                }\n","                lastFrameTime = now;\n","                \n","                try {\n","                    const canvas = document.createElement('canvas');\n","                    canvas.width = video.videoWidth;\n","                    canvas.height = video.videoHeight;\n","                    const ctx = canvas.getContext('2d');\n","                    ctx.drawImage(video, 0, 0);\n","                    \n","                    const imgData = canvas.toDataURL('image/jpeg', 0.8);\n","                    if (imgData && imgData.length > 1000) {\n","                        google.colab.kernel.invokeFunction('notebook.processFrame', [imgData], {});\n","                    }\n","                } catch (e) {\n","                    console.log(e);\n","                }\n","                \n","                requestAnimationFrame(processFrame);\n","            };\n","            \n","            navigator.mediaDevices.getUserMedia({\n","                video: {\n","                    width: {ideal: 640},\n","                    height: {ideal: 480},\n","                    facingMode: 'user',\n","                    frameRate: {ideal: 30}\n","                }\n","            }).then(stream => {\n","                window.cameraActive = true;\n","                video.srcObject = stream;\n","                video.play();\n","                processFrame();\n","            });\n","            "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[19:34:14] üìπ ƒêang x·ª≠ l√Ω video...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","            window.cameraActive = false;\n","            const video = document.querySelector('video');\n","            if (video && video.srcObject) {\n","                video.srcObject.getTracks().forEach(track => track.stop());\n","                video.remove();\n","            }\n","            "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["üõë Camera ƒë√£ t·∫Øt\n","[19:38:22] üõë ƒê√£ d·ª´ng x·ª≠ l√Ω\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"DhULpIXMpyJA"},"execution_count":null,"outputs":[]}]}